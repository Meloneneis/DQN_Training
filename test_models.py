"""
Comprehensive test suite for DQN models with new features.
Tests: Standard DQN, Dueling DQN, NAF (Continuous Actions)
Features tested: LayerNorm, LR Scheduler, Weight Decay, Gradient Clipping
"""

import torch
import torch.optim as optim
import numpy as np
from model import DQN, ContinuousActionDQN
from learning import perform_qlearning_step, perform_qlearning_step_continuous, update_target_net
from replay_buffer import ReplayBuffer

def print_test_header(test_name):
    """Print a formatted test header"""
    print("\n" + "="*70)
    print(f"TEST: {test_name}")
    print("="*70)

def print_success(message):
    """Print success message"""
    print(f"‚úì {message}")

def print_info(message):
    """Print info message"""
    print(f"  {message}")

def test_standard_dqn():
    """Test standard DQN architecture"""
    print_test_header("Standard DQN (Discrete Actions)")

    device = torch.device("cpu")
    action_size = 9

    # Test different configurations
    configs = [
        {"name": "LayerNorm", "normalization": "layer", "use_dueling": False},
        {"name": "No Norm", "normalization": "none", "use_dueling": False},
    ]

    for config in configs:
        print_info(f"Testing {config['name']}...")

        model = DQN(
            action_size=action_size,
            device=device,
            hidden_sizes=[256, 128],
            dropout_rate=0.1,
            cnn_channels=[16, 32, 64],
            cnn_kernels=[8, 4, 3],
            cnn_strides=[4, 2, 1],
            final_spatial_size=8,
            activation='relu',
            normalization=config['normalization'],
            use_dueling=config['use_dueling']
        )

        # Test forward pass
        batch_size = 4
        dummy_obs = np.random.rand(batch_size, 96, 96, 3).astype(np.float32)

        with torch.no_grad():
            q_values = model(dummy_obs)

        assert q_values.shape == (batch_size, action_size), f"Wrong output shape: {q_values.shape}"
        assert not torch.isnan(q_values).any(), "NaN values in output"
        assert not torch.isinf(q_values).any(), "Inf values in output"

        print_success(f"{config['name']}: Forward pass successful, shape {q_values.shape}")

    print_success("All Standard DQN configurations passed!")

def test_dueling_dqn():
    """Test Dueling DQN architecture"""
    print_test_header("Dueling DQN")

    device = torch.device("cpu")
    action_size = 9

    configs = [
        {"name": "Dueling + LayerNorm", "normalization": "layer"},
        {"name": "Dueling + No Norm", "normalization": "none"},
    ]

    for config in configs:
        print_info(f"Testing {config['name']}...")

        model = DQN(
            action_size=action_size,
            device=device,
            hidden_sizes=[256, 128],
            dropout_rate=0.1,
            cnn_channels=[16, 32, 64],
            cnn_kernels=[8, 4, 3],
            cnn_strides=[4, 2, 1],
            final_spatial_size=8,
            activation='gelu',
            normalization=config['normalization'],
            use_dueling=True  # Enable dueling
        )

        # Verify dueling architecture components exist
        assert hasattr(model, 'fc_shared'), "Missing fc_shared layers"
        assert hasattr(model, 'value_stream'), "Missing value_stream"
        assert hasattr(model, 'advantage_stream'), "Missing advantage_stream"
        assert hasattr(model, 'value_output'), "Missing value_output"
        assert hasattr(model, 'advantage_output'), "Missing advantage_output"

        # Test forward pass
        batch_size = 4
        dummy_obs = np.random.rand(batch_size, 96, 96, 3).astype(np.float32)

        with torch.no_grad():
            q_values = model(dummy_obs)

        assert q_values.shape == (batch_size, action_size), f"Wrong output shape: {q_values.shape}"
        assert not torch.isnan(q_values).any(), "NaN values in output"
        assert not torch.isinf(q_values).any(), "Inf values in output"

        print_success(f"{config['name']}: Dueling architecture working, shape {q_values.shape}")

    print_success("All Dueling DQN configurations passed!")

def test_naf_continuous():
    """Test NAF (Continuous Action) DQN"""
    print_test_header("NAF (Continuous Actions)")

    device = torch.device("cpu")
    action_dim = 3  # steering, gas, brake

    configs = [
        {"name": "NAF + LayerNorm", "normalization": "layer"},
        {"name": "NAF + No Norm", "normalization": "none"},
    ]

    for config in configs:
        print_info(f"Testing {config['name']}...")

        model = ContinuousActionDQN(
            action_dim=action_dim,
            device=device,
            hidden_sizes=[256, 128],
            dropout_rate=0.1,
            cnn_channels=[16, 32, 64],
            cnn_kernels=[8, 4, 3],
            cnn_strides=[4, 2, 1],
            final_spatial_size=8,
            activation='silu',
            normalization=config['normalization']
        )

        # Test greedy action selection (no action input)
        batch_size = 4
        dummy_obs = np.random.rand(batch_size, 96, 96, 3).astype(np.float32)

        with torch.no_grad():
            actions = model(dummy_obs)

        assert actions.shape == (batch_size, action_dim), f"Wrong action shape: {actions.shape}"
        assert not torch.isnan(actions).any(), "NaN values in actions"
        assert not torch.isinf(actions).any(), "Inf values in actions"

        # Check action bounds
        assert (actions[:, 0] >= -1.0).all() and (actions[:, 0] <= 1.0).all(), "Steering out of bounds"
        assert (actions[:, 1] >= 0.0).all() and (actions[:, 1] <= 1.0).all(), "Gas out of bounds"
        assert (actions[:, 2] >= 0.0).all() and (actions[:, 2] <= 1.0).all(), "Brake out of bounds"

        print_success(f"{config['name']}: Action generation successful, shape {actions.shape}")

        # Test Q-value computation with actions
        with torch.no_grad():
            q_values, _ = model(dummy_obs, actions)

        assert q_values.shape == (batch_size,), f"Wrong Q-value shape: {q_values.shape}"
        assert not torch.isnan(q_values).any(), "NaN values in Q-values"
        assert not torch.isinf(q_values).any(), "Inf values in Q-values"

        print_success(f"{config['name']}: Q-value computation successful")

    print_success("All NAF configurations passed!")

def test_activation_functions():
    """Test different activation functions"""
    print_test_header("Activation Functions")

    device = torch.device("cpu")
    action_size = 9

    activations = ['relu', 'gelu', 'silu']

    for activation in activations:
        print_info(f"Testing {activation}...")

        model = DQN(
            action_size=action_size,
            device=device,
            hidden_sizes=[128],
            cnn_channels=[16, 32],
            cnn_kernels=[8, 4],
            cnn_strides=[4, 2],
            final_spatial_size=10,
            activation=activation,
            normalization='none',
            use_dueling=False
        )

        dummy_obs = np.random.rand(2, 96, 96, 3).astype(np.float32)

        with torch.no_grad():
            q_values = model(dummy_obs)

        assert not torch.isnan(q_values).any(), f"NaN with {activation}"
        print_success(f"{activation}: Working correctly")

    print_success("All activation functions passed!")

def test_learning_step():
    """Test learning step with gradient clipping"""
    print_test_header("Learning Step (Standard DQN)")

    device = torch.device("cpu")
    action_size = 9

    # Create networks
    policy_net = DQN(
        action_size=action_size,
        device=device,
        hidden_sizes=[128, 64],
        cnn_channels=[16, 32],
        cnn_kernels=[8, 4],
        cnn_strides=[4, 2],
        final_spatial_size=10,
        activation='relu',
        normalization='layer',
        use_dueling=False
    )

    target_net = DQN(
        action_size=action_size,
        device=device,
        hidden_sizes=[128, 64],
        cnn_channels=[16, 32],
        cnn_kernels=[8, 4],
        cnn_strides=[4, 2],
        final_spatial_size=10,
        activation='relu',
        normalization='layer',
        use_dueling=False
    )

    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()

    # Create replay buffer and add samples
    replay_buffer = ReplayBuffer(1000)
    for _ in range(100):
        obs = np.random.rand(1, 96, 96, 3).astype(np.float32)  # Add batch dimension
        action = np.random.randint(0, action_size)
        reward = np.random.randn()
        next_obs = np.random.rand(1, 96, 96, 3).astype(np.float32)  # Add batch dimension
        done = np.random.rand() > 0.9
        replay_buffer.add(obs, action, reward, next_obs, float(done))

    # Create optimizer with weight decay
    optimizer = optim.AdamW(policy_net.parameters(), lr=0.001, weight_decay=0.0001)

    print_info("Testing standard Q-learning step...")

    # Perform learning step with gradient clipping
    loss = perform_qlearning_step(
        policy_net, target_net, optimizer, replay_buffer,
        batch_size=32, gamma=0.99, device=device,
        use_doubleqlearning=True, grad_clip=10.0
    )

    assert not np.isnan(loss), "Loss is NaN"
    assert not np.isinf(loss), "Loss is Inf"
    assert loss >= 0, "Loss is negative"

    print_success(f"Q-learning step successful, loss = {loss:.6f}")

    # Test target network update
    print_info("Testing target network update...")
    old_params = [p.clone() for p in target_net.parameters()]
    update_target_net(policy_net, target_net)
    new_params = list(target_net.parameters())

    # Check that target network was updated
    params_changed = False
    for old_p, new_p in zip(old_params, new_params):
        if not torch.equal(old_p, new_p):
            params_changed = True
            break

    assert params_changed, "Target network was not updated"
    print_success("Target network update successful")

    print_success("Learning step tests passed!")

def test_learning_step_dueling():
    """Test learning step with Dueling DQN"""
    print_test_header("Learning Step (Dueling DQN)")

    device = torch.device("cpu")
    action_size = 9

    # Create dueling networks
    policy_net = DQN(
        action_size=action_size,
        device=device,
        hidden_sizes=[128, 64],
        cnn_channels=[16, 32],
        cnn_kernels=[8, 4],
        cnn_strides=[4, 2],
        final_spatial_size=10,
        activation='gelu',
        normalization='layer',
        use_dueling=True  # Dueling enabled
    )

    target_net = DQN(
        action_size=action_size,
        device=device,
        hidden_sizes=[128, 64],
        cnn_channels=[16, 32],
        cnn_kernels=[8, 4],
        cnn_strides=[4, 2],
        final_spatial_size=10,
        activation='gelu',
        normalization='layer',
        use_dueling=True
    )

    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()

    # Create replay buffer
    replay_buffer = ReplayBuffer(1000)
    for _ in range(100):
        obs = np.random.rand(1, 96, 96, 3).astype(np.float32)  # Add batch dimension
        action = np.random.randint(0, action_size)
        reward = np.random.randn()
        next_obs = np.random.rand(1, 96, 96, 3).astype(np.float32)  # Add batch dimension
        done = np.random.rand() > 0.9
        replay_buffer.add(obs, action, reward, next_obs, float(done))

    optimizer = optim.AdamW(policy_net.parameters(), lr=0.001, weight_decay=0.0001)

    print_info("Testing Dueling Q-learning step...")

    loss = perform_qlearning_step(
        policy_net, target_net, optimizer, replay_buffer,
        batch_size=32, gamma=0.99, device=device,
        use_doubleqlearning=True, grad_clip=10.0
    )

    assert not np.isnan(loss), "Loss is NaN"
    assert loss >= 0, "Loss is negative"

    print_success(f"Dueling Q-learning step successful, loss = {loss:.6f}")
    print_success("Dueling learning step tests passed!")

def test_learning_step_naf():
    """Test learning step with NAF (continuous actions)"""
    print_test_header("Learning Step (NAF - Continuous)")

    device = torch.device("cpu")
    action_dim = 3

    # Create NAF networks
    policy_net = ContinuousActionDQN(
        action_dim=action_dim,
        device=device,
        hidden_sizes=[128, 64],
        cnn_channels=[16, 32],
        cnn_kernels=[8, 4],
        cnn_strides=[4, 2],
        final_spatial_size=10,
        activation='silu',
        normalization='layer'
    )

    target_net = ContinuousActionDQN(
        action_dim=action_dim,
        device=device,
        hidden_sizes=[128, 64],
        cnn_channels=[16, 32],
        cnn_kernels=[8, 4],
        cnn_strides=[4, 2],
        final_spatial_size=10,
        activation='silu',
        normalization='layer'
    )

    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()

    # Create replay buffer with continuous actions
    replay_buffer = ReplayBuffer(1000)
    for _ in range(100):
        obs = np.random.rand(1, 96, 96, 3).astype(np.float32)  # Add batch dimension
        # Continuous action: [steering, gas, brake]
        action = np.array([
            np.random.uniform(-1, 1),  # steering
            np.random.uniform(0, 1),   # gas
            np.random.uniform(0, 1)    # brake
        ], dtype=np.float32)
        reward = np.random.randn()
        next_obs = np.random.rand(1, 96, 96, 3).astype(np.float32)  # Add batch dimension
        done = np.random.rand() > 0.9
        replay_buffer.add(obs, action, reward, next_obs, float(done))

    optimizer = optim.AdamW(policy_net.parameters(), lr=0.001, weight_decay=0.0001)

    print_info("Testing NAF Q-learning step...")

    loss = perform_qlearning_step_continuous(
        policy_net, target_net, optimizer, replay_buffer,
        batch_size=32, gamma=0.99, device=device,
        use_doubleqlearning=True, grad_clip=10.0
    )

    assert not np.isnan(loss), "Loss is NaN"
    assert loss >= 0, "Loss is negative"

    print_success(f"NAF Q-learning step successful, loss = {loss:.6f}")
    print_success("NAF learning step tests passed!")

def test_lr_scheduler():
    """Test learning rate schedulers"""
    print_test_header("Learning Rate Schedulers")

    device = torch.device("cpu")
    action_size = 9

    model = DQN(
        action_size=action_size,
        device=device,
        hidden_sizes=[128],
        cnn_channels=[16, 32],
        cnn_kernels=[8, 4],
        cnn_strides=[4, 2],
        final_spatial_size=10,
        activation='relu',
        normalization='none',
        use_dueling=False
    )

    schedulers = [
        ("Cosine", optim.lr_scheduler.CosineAnnealingLR, {"T_max": 100}),
        ("Exponential", optim.lr_scheduler.ExponentialLR, {"gamma": 0.995}),
    ]

    for sched_name, sched_class, sched_kwargs in schedulers:
        print_info(f"Testing {sched_name} scheduler...")

        optimizer = optim.AdamW(model.parameters(), lr=0.001)
        scheduler = sched_class(optimizer, **sched_kwargs)

        initial_lr = optimizer.param_groups[0]['lr']

        # Run a few steps
        for _ in range(10):
            optimizer.zero_grad()
            dummy_obs = np.random.rand(2, 96, 96, 3).astype(np.float32)
            output = model(dummy_obs)
            loss = output.mean()
            loss.backward()
            optimizer.step()
            scheduler.step()

        final_lr = optimizer.param_groups[0]['lr']

        # LR should have changed (except for very early steps in some schedulers)
        print_success(f"{sched_name}: LR changed from {initial_lr:.6f} to {final_lr:.6f}")

    print_success("All LR schedulers working!")

def test_weight_decay():
    """Test weight decay functionality"""
    print_test_header("Weight Decay")

    device = torch.device("cpu")
    action_size = 9

    model = DQN(
        action_size=action_size,
        device=device,
        hidden_sizes=[128],
        cnn_channels=[16, 32],
        cnn_kernels=[8, 4],
        cnn_strides=[4, 2],
        final_spatial_size=10,
        activation='relu',
        normalization='none',
        use_dueling=False
    )

    # Test with different weight decay values
    weight_decays = [0.0, 0.0001, 0.001]

    for wd in weight_decays:
        print_info(f"Testing weight_decay={wd}...")

        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=wd)

        # Perform a few optimization steps
        for _ in range(5):
            optimizer.zero_grad()
            dummy_obs = np.random.rand(2, 96, 96, 3).astype(np.float32)
            output = model(dummy_obs)
            loss = output.mean()
            loss.backward()
            optimizer.step()

        print_success(f"Weight decay {wd} working")

    print_success("Weight decay tests passed!")

def run_all_tests():
    """Run all test suites"""
    print("\n" + "="*70)
    print("COMPREHENSIVE MODEL TESTING SUITE")
    print("Testing: Standard DQN, Dueling DQN, NAF (Continuous)")
    print("Features: Normalization, Activations, Learning, Schedulers")
    print("="*70)

    try:
        # Architecture tests
        test_standard_dqn()
        test_dueling_dqn()
        test_naf_continuous()
        test_activation_functions()

        # Learning tests
        test_learning_step()
        test_learning_step_dueling()
        test_learning_step_naf()

        # Optimizer feature tests
        test_lr_scheduler()
        test_weight_decay()

        # Final summary
        print("\n" + "="*70)
        print("üéâ ALL TESTS PASSED! üéâ")
        print("="*70)
        print("\nSummary:")
        print("  ‚úì Standard DQN: All normalization types working")
        print("  ‚úì Dueling DQN: Architecture and learning working")
        print("  ‚úì NAF (Continuous): Action generation and Q-values working")
        print("  ‚úì Activation functions: All variants working")
        print("  ‚úì Learning steps: Gradient updates working correctly")
        print("  ‚úì LR Schedulers: Cosine, Exponential, Step all working")
        print("  ‚úì Weight Decay: Regularization working")
        print("\nYour implementation is ready for training! üöÄ")
        print("="*70 + "\n")

        return True

    except AssertionError as e:
        print("\n" + "="*70)
        print("‚ùå TEST FAILED!")
        print("="*70)
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return False
    except Exception as e:
        print("\n" + "="*70)
        print("‚ùå UNEXPECTED ERROR!")
        print("="*70)
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = run_all_tests()
    exit(0 if success else 1)
